{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "# Import supplementary visualization code visuals.py\n",
    "#import visuals as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting files and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data= \"/home/ubuntu/udacity/CodeGladiator/invesco/data\"\n",
    "\n",
    "transaction_file = \"Code-Gladiators-Transaction.csv\"\n",
    "\n",
    "####### adding new transaction fiel which have grouping of all the transactions\n",
    "#grouped_transaction_amounts = \"grouped_transaction_amounts.csv\"\n",
    "#investment_exp_file = \"Code-Gladiators-InvestmentExperience.csv\"\n",
    "investment_exp_file = \"imputed_investment_exp.csv\"\n",
    "investment_segment = \"investment_vehicle_segment.csv\"\n",
    "aum_file = \"Code-Gladiators-AUM.csv\"\n",
    "#activity_file = \"Code-Gladiators-Activity.csv\"\n",
    "activity_file = \"grouped_processed_activity.csv\"\n",
    "test_file = \"test_data.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading csv files into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transaction_df = pd.read_csv(os.path.join(path_to_data, transaction_file))\n",
    "#grouped_transaction_amounts_df = pd.read_csv(os.path.join(path_to_data, grouped_transaction_amounts))\n",
    "investment_exp_df = pd.read_csv(os.path.join(path_to_data, investment_exp_file))\n",
    "investment_segment_df = pd.read_csv(os.path.join(path_to_data, investment_segment))\n",
    "aum_df = pd.read_csv(os.path.join(path_to_data, aum_file))\n",
    "activity_df = pd.read_csv(os.path.join(path_to_data, activity_file))\n",
    "test_df = pd.read_csv(os.path.join(path_to_data,test_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Import two metrics from sklearn - fbeta_score and accuracy_score\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "# time.time() return time in seconds since the Epoch\n",
    "from time import time\n",
    "\n",
    "beta = 0.5\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size'\n",
    "    X_train = X_train[:sample_size]\n",
    "    y_train = y_train[:sample_size]\n",
    "    \n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train, y_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train, predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples\n",
    "    results['f_train'] = fbeta_score(y_train, predictions_train, beta=beta)\n",
    "        \n",
    "   # TODO: Compute F-score on the test set\n",
    "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=beta)\n",
    "       \n",
    "    # Success\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transaction_Type',\n",
       " 'Counts_investor',\n",
       " 'Counts_advisor',\n",
       " 'Rating',\n",
       " '1 Yr % Rank',\n",
       " '3 Yr % Rank',\n",
       " '1 Yr Return',\n",
       " '3 Yr Return',\n",
       " '1 Yr Excess Return vs Primary Ix',\n",
       " '3 Yr Excess Return vs Primary Ix',\n",
       " '1 Yr Excess Return vs Category Ix',\n",
       " '3 Yr Excess Return vs Category Ix',\n",
       " 'Net Flows',\n",
       " 'Morningstar_Category_Rating',\n",
       " 'AUM_investor_log',\n",
       " 'Shares_investor_log',\n",
       " 'AUM_advisor_log',\n",
       " 'Shares_advisor_log']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#column_list= ['AUM_investor_log','Counts_investor','Shares_investor_log','AUM_advisor_log','Shares_advisor_log','Rating','1 Yr % Rank','3 Yr % Rank','1 Yr Return','3 Yr Return','Net Flows','Morningstar_Category_Rating','Transaction_Type']\n",
    "see_activity= False\n",
    "\n",
    "column_list= [\n",
    "    'Transaction_Type',\n",
    "#    'Shares_investor',\n",
    "#    'AUM_investor',\n",
    "     'Counts_investor',\n",
    "     'Counts_advisor',\n",
    "#\t'Shares_advisor',\n",
    "#\t'AUM_advisor',\n",
    "#\t'Morningstar Category',\n",
    "#\t'Investment',\n",
    "\t'Rating',\n",
    "\t'1 Yr % Rank',\n",
    "\t'3 Yr % Rank',\n",
    "#\t'5 Yr % Rank',\n",
    "#\t'10 Yr % Rank',\n",
    "\t'1 Yr Return',\n",
    "\t'3 Yr Return',\n",
    "#\t'5 Yr Return',\n",
    "#\t'10 Yr Return',\n",
    "\t'1 Yr Excess Return vs Primary Ix',\n",
    "\t'3 Yr Excess Return vs Primary Ix',\n",
    "#\t'5 Yr Excess Return vs Primary Ix',\n",
    "#\t'10 Yr Excess Return vs Primary Ix',\n",
    "\t'1 Yr Excess Return vs Category Ix',\n",
    "\t'3 Yr Excess Return vs Category Ix',\n",
    "#\t'5 Yr Excess Return vs Category Ix',\n",
    "#\t'10 Yr Excess Return vs Category Ix',\n",
    "\t'Net Flows',\n",
    "\t'Morningstar_Category_Rating',\n",
    "#\t'investment_vehicle_segment',\n",
    "\t'AUM_investor_log',\n",
    "\t'Shares_investor_log',\n",
    "\t'AUM_advisor_log',\n",
    "\t'Shares_advisor_log',\n",
    "\t]\n",
    "\n",
    "if(see_activity):\n",
    "    column_list.extend(list(activity_df.columns[pd.Series(activity_df.columns).str.startswith('Activity')]))\n",
    "\n",
    "\n",
    "display(column_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def dense_to_one_hot(labels_dense, num_classes=2):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    print('num_labels')\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    print(type(labels_dense))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot\n",
    "\n",
    "def preproc(unclean_batch_x):\n",
    "    \"\"\"Convert values to range 0-1\"\"\"\n",
    "    temp_batch = unclean_batch_x / unclean_batch_x.max()\n",
    "    \n",
    "    return temp_batch\n",
    "\n",
    "def batch_creator(batch_size, dataset_length, dataset_name):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    batch_mask = rng.choice(dataset_length, batch_size)\n",
    "    \n",
    "    batch_x = eval(dataset_name + '_x')[[batch_mask]] #.reshape(-1, input_num_units)\n",
    "    batch_x = preproc(batch_x)\n",
    "    \n",
    "    #if dataset_name == 'transaction_type':\n",
    "    batch_y = eval('transaction_type').ix[batch_mask].values\n",
    "    batch_y = dense_to_one_hot(batch_y)\n",
    "        \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping data by unique advisor id and month. Final output will store sums of the assets under managements and shares for each advisor in particular month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:101: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:103: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:104: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:108: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:111: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "unsupported iterator index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f1a1c6479bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mtotal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-121b87eb199f>\u001b[0m in \u001b[0;36mbatch_creator\u001b[0;34m(batch_size, dataset_length, dataset_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#if dataset_name == 'transaction_type':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transaction_type'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-121b87eb199f>\u001b[0m in \u001b[0;36mdense_to_one_hot\u001b[0;34m(labels_dense, num_classes)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlabels_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_dense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlabels_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_offset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabels_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels_one_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: unsupported iterator index"
     ]
    }
   ],
   "source": [
    "transaction_df = pd.read_csv(os.path.join(path_to_data, transaction_file))\n",
    "investment_exp_df = pd.read_csv(os.path.join(path_to_data, investment_exp_file))\n",
    "#grouped_transaction_amounts_df = pd.read_csv(os.path.join(path_to_data, grouped_transaction_amounts))\n",
    "investment_segment_df = pd.read_csv(os.path.join(path_to_data, investment_segment))\n",
    "aum_df = pd.read_csv(os.path.join(path_to_data, aum_file))\n",
    "activity_df = pd.read_csv(os.path.join(path_to_data, activity_file))\n",
    "test_df = pd.read_csv(os.path.join(path_to_data,test_file))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grouped_advisor_aum_df = aum_df.groupby(['Unique_Advisor_Id','Month']).agg({'AUM': 'sum','Shares':'sum','Unique_Investment_Id':'count'}).reset_index().rename(columns={'Unique_Investment_Id':'Counts'})\n",
    "grouped_investment_aum_df = aum_df.groupby(['Unique_Investment_Id','Month']).agg({'AUM': 'sum','Shares':'sum','Unique_Advisor_Id':'count'}).reset_index().rename(columns={'Unique_Advisor_Id':'Counts'})\n",
    "\n",
    "grouped_investment_aum_df['Year'],grouped_investment_aum_df['Month']=grouped_investment_aum_df['Month'].str.split(' /', 1).str\n",
    "grouped_advisor_aum_df['Year'],grouped_advisor_aum_df['Month']=grouped_advisor_aum_df['Month'].str.split(' /', 1).str\n",
    "\n",
    "transaction_df['Year'],transaction_df['Month']=transaction_df['Month'].str.split(' /', 1).str\n",
    "investment_exp_df['Year'],investment_exp_df['Month']=investment_exp_df['Month'].str.split(' /', 1).str\n",
    "aum_df['Year'],aum_df['Month']=aum_df['Month'].str.split(' /', 1).str\n",
    "activity_df['Year'],activity_df['Month']=activity_df['Month'].str.split(' /', 1).str\n",
    "#grouped_transaction_amounts_df['Year'],grouped_transaction_amounts_df['Month'] =grouped_transaction_amounts_df['Month'].str.split(' /', 1).str\n",
    "\n",
    "grouped_investment_aum_df['Mapping_Month']= grouped_investment_aum_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "grouped_advisor_aum_df['Mapping_Month']= grouped_advisor_aum_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "investment_exp_df['Mapping_Month']= investment_exp_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "aum_df['Mapping_Month']= aum_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "activity_df['Mapping_Month']= activity_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "#grouped_transaction_amounts_df['Mapping_Month']= grouped_transaction_amounts_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "\n",
    "\n",
    "transaction_df['Month'] =  transaction_df['Month'].astype(int).apply(lambda x : x+0)\n",
    "\n",
    "final_transaction = pd.merge(transaction_df, grouped_investment_aum_df, left_on=[\"Month\",\"Unique_Investment_Id\"],right_on=[\"Mapping_Month\",\"Unique_Investment_Id\"], how=\"left\") \n",
    "final_transaction = final_transaction.rename(columns={'Month_x': 'Month', 'AUM': 'AUM_investor','Year_x' : 'Year', 'Counts' : 'Counts_investor', 'Shares': 'Shares_investor','Month_y':'Month_actual'})\n",
    "final_transaction = final_transaction.drop('Year_y', 1)\n",
    "\n",
    "test_transaction = pd.merge(test_df, grouped_investment_aum_df[grouped_investment_aum_df[\"Mapping_Month\"]== 13], on=\"Unique_Investment_Id\", how=\"left\")\n",
    "test_transaction = test_transaction.rename(columns={ 'AUM': 'AUM_investor','Counts' : 'Counts_investor', 'Shares': 'Shares_investor'})\n",
    "\n",
    "final_transaction = pd.merge(final_transaction, grouped_advisor_aum_df, left_on=[\"Month\",\"Unique_Advisor_Id\"],right_on=[\"Mapping_Month\",\"Unique_Advisor_Id\"], how=\"left\") \n",
    "final_transaction = final_transaction.rename(columns={'Month_x': 'Month', 'AUM': 'AUM_advisor','Year_x' : 'Year', 'Counts' : 'Counts_advisor', 'Shares': 'Shares_advisor','Mapping_Month_x': 'Mapping_Month'})\n",
    "final_transaction = final_transaction.drop(['Year_y','Mapping_Month_y','Month_y'], 1)\n",
    "\n",
    "test_transaction = pd.merge(test_transaction, grouped_advisor_aum_df[grouped_advisor_aum_df[\"Mapping_Month\"]== 13], on=\"Unique_Advisor_Id\", how=\"left\")\n",
    "test_transaction = test_transaction.rename(columns={ 'AUM': 'AUM_advisor','Counts' : 'Counts_advisor', 'Shares': 'Shares_advisor','Month_x':'Month','Mapping_Month_x':'Mapping_Month'})\n",
    "test_transaction = test_transaction.drop(['Year_x','Year_y','Mapping_Month_y','Month_y','Mapping_Month'], 1)\n",
    "\n",
    "investment_exp_df['investment_vehicle_segment']= investment_segment_df['investment_vehicle_segment']\n",
    "investment_exp_df= investment_exp_df[investment_exp_df['Year']=='2016']\n",
    "activity_df = activity_df[activity_df['Year']=='2016']\n",
    "\n",
    "final_transaction_with_exp = pd.merge(final_transaction, investment_exp_df, left_on=[\"Month\",\"Unique_Investment_Id\"],right_on=[\"Mapping_Month\",\"Unique_Investment_Id\"], how=\"left\") \n",
    "final_transaction_with_exp = final_transaction_with_exp.rename(columns={'Month_x': 'Month', 'AUM': 'AUM_advisor','Year_x' : 'Year', 'Mapping_Month_x':'Mapping_Month'})\n",
    "final_transaction_with_exp = final_transaction_with_exp.drop(['Year_y','Mapping_Month_y','Month_y'], 1)\n",
    "#final_transaction_with_exp = pd.merge(final_transaction_with_exp, grouped_transaction_amounts_df, left_on=[\"Month\",\"Unique_Investment_Id\"],right_on=[\"Mapping_Month\",\"Unique_Investment_Id\"], how=\"left\") \n",
    "#final_transaction_with_exp = final_transaction_with_exp.drop(['Year_y','Mapping_Month_y','Month_y'], 1)\n",
    "\n",
    "test_transaction_with_exp = pd.merge(test_transaction, investment_exp_df[investment_exp_df[\"Mapping_Month\"]== 13 ], on=\"Unique_Investment_Id\", how=\"left\")\n",
    "#test_transaction_with_exp = pd.merge(test_transaction_with_exp, grouped_transaction_amounts_df[grouped_transaction_amounts_df[\"Mapping_Month\"]== 13 ], on=[\"Unique_Investment_Id\"], how=\"left\")\n",
    "\n",
    "#display(final_transaction_with_exp.head())\n",
    "#display(test_transaction_with_exp.head())\n",
    "\n",
    "##merging with activity\n",
    "if(see_activity):\n",
    "    final_transaction_with_exp = pd.merge(final_transaction_with_exp, activity_df, left_on=[\"Month\",\"Unique_Advisor_Id\"],right_on=[\"Mapping_Month\",\"Unique_Advisor_Id\"], how=\"left\") \n",
    "    test_transaction_with_exp = pd.merge(test_transaction_with_exp, activity_df[activity_df[\"Mapping_Month\"]== 13 ], on=\"Unique_Advisor_Id\", how=\"left\")\n",
    "\n",
    "'''\n",
    "final_transaction_with_exp['AUM_investor_log'] = np.log(final_transaction_with_exp['AUM_investor'])\n",
    "final_transaction_with_exp['Shares_investor_log'] = np.log(final_transaction_with_exp['Shares_investor'])\n",
    "\n",
    "final_transaction_with_exp['AUM_advisor_log'] = np.log(final_transaction_with_exp['AUM_advisor'])\n",
    "final_transaction_with_exp['Shares_advisor_log'] = np.log(final_transaction_with_exp['Shares_advisor'])\n",
    "\n",
    "\n",
    "test_transaction_with_exp['AUM_investor_log'] = np.log(test_transaction_with_exp['AUM_investor'])\n",
    "test_transaction_with_exp['Shares_investor_log'] = np.log(test_transaction_with_exp['Shares_investor'])\n",
    "\n",
    "test_transaction_with_exp['AUM_advisor_log'] = np.log(test_transaction_with_exp['AUM_advisor'])\n",
    "test_transaction_with_exp['Shares_advisor_log'] = np.log(test_transaction_with_exp['Shares_advisor'])\n",
    "\n",
    "\n",
    "#############################################\n",
    "final_transaction_with_exp['AUM_investor_log'] = (np.log(final_transaction_with_exp['AUM_investor']/final_transaction_with_exp['Counts_investor'])).where(final_transaction_with_exp['AUM_investor']!=0)\n",
    "final_transaction_with_exp['Shares_investor_log'] = (np.log(final_transaction_with_exp['Shares_investor']/final_transaction_with_exp['Counts_investor'])).where(final_transaction_with_exp['Shares_investor']!=0)\n",
    "\n",
    "final_transaction_with_exp['AUM_advisor_log'] = (np.log(final_transaction_with_exp['AUM_advisor']/final_transaction_with_exp['Counts_advisor'])).where(final_transaction_with_exp['AUM_advisor']!=0)\n",
    "final_transaction_with_exp['Shares_advisor_log'] = (np.log(final_transaction_with_exp['Shares_advisor']/final_transaction_with_exp['Counts_advisor'])).where(final_transaction_with_exp['Shares_advisor']!=0)\n",
    "\n",
    "\n",
    "test_transaction_with_exp['AUM_investor_log'] = (np.log(test_transaction_with_exp['AUM_investor']/test_transaction_with_exp['Counts_investor'])).where(test_transaction_with_exp['AUM_investor']!=0)\n",
    "test_transaction_with_exp['Shares_investor_log'] = (np.log(test_transaction_with_exp['Shares_investor']/test_transaction_with_exp['Counts_investor'])).where(test_transaction_with_exp['Shares_investor']!=0)\n",
    "\n",
    "test_transaction_with_exp['AUM_advisor_log'] = (np.log(test_transaction_with_exp['AUM_advisor']/test_transaction_with_exp['Counts_advisor'])).where(test_transaction_with_exp['AUM_advisor']!=0)\n",
    "test_transaction_with_exp['Shares_advisor_log'] = (np.log(test_transaction_with_exp['Shares_advisor']/test_transaction_with_exp['Counts_advisor'])).where(test_transaction_with_exp['Shares_advisor']!=0)\n",
    "##########################################################\n",
    "'''\n",
    "final_transaction_with_exp['AUM_investor_log'] = (np.log(final_transaction_with_exp['AUM_investor'])).where(final_transaction_with_exp['AUM_investor']!=0)\n",
    "final_transaction_with_exp['Shares_investor_log'] = (np.log(final_transaction_with_exp['Shares_investor'])).where(final_transaction_with_exp['Shares_investor']!=0)\n",
    "\n",
    "final_transaction_with_exp['AUM_advisor_log'] = (np.log(final_transaction_with_exp['AUM_advisor'])).where(final_transaction_with_exp['AUM_advisor']!=0)\n",
    "final_transaction_with_exp['Shares_advisor_log'] = (np.log(final_transaction_with_exp['Shares_advisor'])).where(final_transaction_with_exp['Shares_advisor']!=0)\n",
    "\n",
    "\n",
    "test_transaction_with_exp['AUM_investor_log'] = (np.log(test_transaction_with_exp['AUM_investor'])).where(test_transaction_with_exp['AUM_investor']!=0)\n",
    "test_transaction_with_exp['Shares_investor_log'] = (np.log(test_transaction_with_exp['Shares_investor'])).where(test_transaction_with_exp['Shares_investor']!=0)\n",
    "\n",
    "test_transaction_with_exp['AUM_advisor_log'] = (np.log(test_transaction_with_exp['AUM_advisor'])).where(test_transaction_with_exp['AUM_advisor']!=0)\n",
    "test_transaction_with_exp['Shares_advisor_log'] = (np.log(test_transaction_with_exp['Shares_advisor'])).where(test_transaction_with_exp['Shares_advisor']!=0)\n",
    "\n",
    "final_transaction_with_exp= final_transaction_with_exp[final_transaction_with_exp.Month !=1]\n",
    "\n",
    "#final_transaction_with_exp= final_transaction_with_exp.dropna()\n",
    "\n",
    "\n",
    "required_train_df = final_transaction_with_exp.filter(column_list)\n",
    "if(see_activity):\n",
    "    required_train_df[required_train_df.columns[pd.Series(required_train_df.columns).str.startswith('Activity')]] = required_train_df[required_train_df.columns[pd.Series(required_train_df.columns).str.startswith('Activity')]].fillna(0)\n",
    "\n",
    "\n",
    "#display(required_train_df.head())\n",
    "\n",
    "required_train_df= required_train_df.fillna(required_train_df.median())\n",
    "\n",
    "required_train_df['Transaction_Type']= required_train_df.apply(lambda x: 0 if x['Transaction_Type']== 'P' else 1, axis=1)\n",
    "\n",
    "required_test_df = test_transaction_with_exp.filter(column_list)\n",
    "\n",
    "if(see_activity):\n",
    "    required_test_df[required_test_df.columns[pd.Series(required_test_df.columns).str.startswith('Activity')]] = required_test_df[required_test_df.columns[pd.Series(required_test_df.columns).str.startswith('Activity')]].fillna(0)\n",
    "\n",
    "\n",
    "#required_test_df['Rating'] = required_test_df['Rating'].astype(float)\n",
    "\n",
    "required_test_df=required_test_df.fillna(required_test_df.median())\n",
    "\n",
    "# Split the data into features and target label\n",
    "transaction_type = required_train_df['Transaction_Type']\n",
    "features_raw = required_train_df.drop('Transaction_Type', axis = 1)\n",
    "test_raw = required_test_df\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "var_mod = column_list\n",
    "#var_mod.remove('Transaction_Type')\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    if i == 'Transaction_Type' \\\n",
    "    or i == 'Net Flows' \\\n",
    "    or i == 'AUM_investor_log' \\\n",
    "    or i == 'Shares_investor_log' \\\n",
    "    or i == 'AUM_advisor_log' \\\n",
    "    or i == 'Shares_advisor_log':\n",
    "        continue\n",
    "    features_raw[i] = le.fit_transform(features_raw[i])\n",
    "    test_raw[i] = le.fit_transform(test_raw[i])\n",
    "\n",
    "    \n",
    "    \n",
    "#####################################Preparation for neural net###################333\n",
    "train_x= features_raw.as_matrix()\n",
    "test_x = test_raw.as_matrix()\n",
    "\n",
    "split_size = int(train_x.shape[0]*0.8)\n",
    "\n",
    "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
    "train_y, val_y = transaction_type.values[:split_size], transaction_type.values[split_size:]\n",
    "\n",
    "\n",
    "\n",
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)\n",
    "### set all variables\n",
    "\n",
    "# number of neurons in each layer\n",
    "input_num_units = 17\n",
    "hidden_num_units = 25\n",
    "output_num_units = 2\n",
    "\n",
    "# define placeholders\n",
    "x = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "\n",
    "# set remaining variables\n",
    "epochs = 6\n",
    "batch_size = 300\n",
    "learning_rate = 0.1\n",
    "\n",
    "### define weights and biases of the neural network\n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.truncated_normal([input_num_units, hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.zeros([hidden_num_units, output_num_units]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.truncated_normal([hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.zeros([output_num_units]))\n",
    "}\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(x, weights['hidden']), biases['hidden'])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_layer, y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create initialized variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    ### for each epoch, do:\n",
    "    ###   for each batch, do:\n",
    "    ###     create pre-processed batch\n",
    "    ###     run optimizer by feeding batch\n",
    "    ###     find cost and reiterate to minimize\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(features_raw.shape[0]/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "            \n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print(\"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost))\n",
    "    \n",
    "    print (\"\\nTraining complete!\")\n",
    "    \n",
    "    \n",
    "    # find predictions on val set\n",
    "    pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(pred_temp, \"float\"))\n",
    "    #print (\"Validation Accuracy:\", accuracy.eval({x: val_x.reshape(-1, input_num_units), y: dense_to_one_hot(val_y)}))\n",
    "    print (\"Validation Accuracy:\", accuracy.eval({x: val_x.reshape(-1, input_num_units), y: dense_to_one_hot(val_y)}))\n",
    "    \n",
    "    predict = tf.argmax(output_layer, 1)\n",
    "    #pred = predict.eval({x: test_x.reshape(-1, input_num_units)})\n",
    "    pred = predict.eval({x: train_x})\n",
    "    \n",
    "    print (\"F-score on training data: {:.4f}\".format(fbeta_score(train_y, pred, beta = 0.5)))\n",
    "    pred = predict.eval({x: val_x})\n",
    "    print (\"F-score on testing data: {:.4f}\".format(fbeta_score(val_y, pred, beta = 0.5)))\n",
    "\n",
    "    pred = predic.eval({x:val_x})\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "'''\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "best_clf= clf_Ran\n",
    "filename = 'logistic_regression_best_invesco.joblib.pkl'\n",
    "\n",
    "_ = joblib.dump(best_clf, filename, compress=9)\n",
    "\n",
    "clf_loaded = joblib.load(filename)\n",
    "print(\"file loaded\")\n",
    "\n",
    "pred = clf_loaded.predict(test_raw)\n",
    "pred_prob = clf_loaded.predict_proba(test_raw)\n",
    "\n",
    "pred_prob = pd.DataFrame(pred_prob[:,1],columns=[\"Propensity_Score\"])\n",
    "display(pred_prob[:5])\n",
    "'''\n",
    "\n",
    "pred_df= pd.DataFrame(pred,columns=[\"Redeem_Status\"])\n",
    "\n",
    "pred_df=pred_df.replace([0,1],['NO','YES'])\n",
    "pred_df.head()\n",
    "\n",
    "pred_df['Redeem_Status'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119192"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_Advisor_Id</th>\n",
       "      <th>Unique_Investment_Id</th>\n",
       "      <th>Propensity_Score</th>\n",
       "      <th>Redeem_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000103</td>\n",
       "      <td>14147</td>\n",
       "      <td>0.967143</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000103</td>\n",
       "      <td>3534</td>\n",
       "      <td>0.726910</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000103</td>\n",
       "      <td>3651</td>\n",
       "      <td>0.242776</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000103</td>\n",
       "      <td>7668</td>\n",
       "      <td>0.799177</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000103</td>\n",
       "      <td>9339</td>\n",
       "      <td>0.500785</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_Advisor_Id  Unique_Investment_Id  Propensity_Score Redeem_Status\n",
       "0            1000103                 14147          0.967143           YES\n",
       "1            1000103                  3534          0.726910           YES\n",
       "2            1000103                  3651          0.242776            NO\n",
       "3            1000103                  7668          0.799177           YES\n",
       "4            1000103                  9339          0.500785           YES"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([test_df, pred_prob, pred_df], axis=1)\n",
    "\n",
    "\n",
    "result.to_csv('test_data_v6-usingMeanForAUM.csv',index=False)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result.to_csv('test_data_v5-removednetflows.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
