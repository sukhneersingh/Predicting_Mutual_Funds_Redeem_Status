{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Importing required libraries\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import numpy as np\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting files and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data= \"/home/ubuntu/udacity/CodeGladiator/invesco/data\"\n",
    "\n",
    "transaction_file = \"Code-Gladiators-Transaction.csv\"\n",
    "\n",
    "# using imputed file for investment experience which has missing values imputed  with mean of particular investment ID\n",
    "investment_exp_file = \"imputed_investment_exp.csv\"\n",
    "investment_segment = \"investment_vehicle_segment.csv\"\n",
    "aum_file = \"Code-Gladiators-AUM.csv\"\n",
    "activity_file = \"Code-Gladiators-Activity.csv\"\n",
    "\n",
    "test_file = \"test_data.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading csv files into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transaction_df = pd.read_csv(os.path.join(path_to_data, transaction_file))\n",
    "investment_exp_df = pd.read_csv(os.path.join(path_to_data, investment_exp_file))\n",
    "investment_segment_df = pd.read_csv(os.path.join(path_to_data, investment_segment))\n",
    "aum_df = pd.read_csv(os.path.join(path_to_data, aum_file))\n",
    "activity_df = pd.read_csv(os.path.join(path_to_data, activity_file))\n",
    "test_df = pd.read_csv(os.path.join(path_to_data,test_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Import two metrics from sklearn - fbeta_score and accuracy_score\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "# time.time() return time in seconds since the Epoch\n",
    "from time import time\n",
    "\n",
    "beta = 0.5\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size'\n",
    "    X_train = X_train[:sample_size]\n",
    "    y_train = y_train[:sample_size]\n",
    "    \n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train, y_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train, predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples\n",
    "    results['f_train'] = fbeta_score(y_train, predictions_train, beta=beta)\n",
    "        \n",
    "   # TODO: Compute F-score on the test set\n",
    "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=beta)\n",
    "       \n",
    "    # Success\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the required columns for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_list= [\n",
    "    'Transaction_Type',\n",
    "#    'Shares_investor',\n",
    "#    'AUM_investor',\n",
    "     'Counts_investor',\n",
    "#     'Counts_advisor',\n",
    "#\t'Shares_advisor',\n",
    "#\t'AUM_advisor',\n",
    "#\t'Morningstar Category',\n",
    "#\t'Investment',\n",
    "\t'Rating',\n",
    "\t'1 Yr % Rank',\n",
    "\t'3 Yr % Rank',\n",
    "#\t'5 Yr % Rank',\n",
    "#\t'10 Yr % Rank',\n",
    "\t'1 Yr Return',\n",
    "\t'3 Yr Return',\n",
    "#\t'5 Yr Return',\n",
    "#\t'10 Yr Return',\n",
    "\t'1 Yr Excess Return vs Primary Ix',\n",
    "\t'3 Yr Excess Return vs Primary Ix',\n",
    "#\t'5 Yr Excess Return vs Primary Ix',\n",
    "#\t'10 Yr Excess Return vs Primary Ix',\n",
    "\t'1 Yr Excess Return vs Category Ix',\n",
    "\t'3 Yr Excess Return vs Category Ix',\n",
    "#\t'5 Yr Excess Return vs Category Ix',\n",
    "#\t'10 Yr Excess Return vs Category Ix',\n",
    "\t'Net Flows',\n",
    "\t'Morningstar_Category_Rating',\n",
    "#\t'investment_vehicle_segment',\n",
    "\t'AUM_investor_log',\n",
    "\t'Shares_investor_log',\n",
    "\t'AUM_advisor_log',\n",
    "\t'Shares_advisor_log',\n",
    "\t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing data and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping data by unique advisor id and month. Final output will store sums of the assets under managements and shares for each advisor in particular month. Then that data is combined iwht experiences and imputed with mean for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:60: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:62: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:63: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:67: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:70: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 130977 samples.\n",
      "Testing set has 32745 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/scipy/optimize/linesearch.py:414: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 130977 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': {0: {'acc_test': 0.63689112841655215,\n",
       "   'acc_train': 0.63543217511471484,\n",
       "   'f_test': 0.6756056375704278,\n",
       "   'f_train': 0.67400239212976776,\n",
       "   'pred_time': 0.007798194885253906,\n",
       "   'train_time': 21.428923845291138}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Propensity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.979496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.645041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.233225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.392099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Propensity_Score\n",
       "0          0.979496\n",
       "1          0.645041\n",
       "2          0.233225\n",
       "3          0.800757\n",
       "4          0.392099"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "YES    8091\n",
       "NO      623\n",
       "Name: Redeem_Status, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading files\n",
    "\n",
    "transaction_df = pd.read_csv(os.path.join(path_to_data, transaction_file))\n",
    "investment_exp_df = pd.read_csv(os.path.join(path_to_data, investment_exp_file))\n",
    "investment_segment_df = pd.read_csv(os.path.join(path_to_data, investment_segment))\n",
    "aum_df = pd.read_csv(os.path.join(path_to_data, aum_file))\n",
    "activity_df = pd.read_csv(os.path.join(path_to_data, activity_file))\n",
    "test_df = pd.read_csv(os.path.join(path_to_data,test_file))\n",
    "\n",
    "\n",
    "#grouping data\n",
    "\n",
    "grouped_advisor_aum_df = aum_df.groupby(['Unique_Advisor_Id','Month']).agg({'AUM': 'sum','Shares':'sum','Unique_Investment_Id':'count'}).reset_index().rename(columns={'Unique_Investment_Id':'Counts'})\n",
    "grouped_investment_aum_df = aum_df.groupby(['Unique_Investment_Id','Month']).agg({'AUM': 'sum','Shares':'sum','Unique_Advisor_Id':'count'}).reset_index().rename(columns={'Unique_Advisor_Id':'Counts'})\n",
    "\n",
    "grouped_investment_aum_df['Year'],grouped_investment_aum_df['Month']=grouped_investment_aum_df['Month'].str.split(' /', 1).str\n",
    "grouped_advisor_aum_df['Year'],grouped_advisor_aum_df['Month']=grouped_advisor_aum_df['Month'].str.split(' /', 1).str\n",
    "\n",
    "# seperating month from Month column\n",
    "transaction_df['Year'],transaction_df['Month']=transaction_df['Month'].str.split(' /', 1).str\n",
    "investment_exp_df['Year'],investment_exp_df['Month']=investment_exp_df['Month'].str.split(' /', 1).str\n",
    "aum_df['Year'],aum_df['Month']=aum_df['Month'].str.split(' /', 1).str\n",
    "activity_df['Year'],activity_df['Month']=activity_df['Month'].str.split(' /', 1).str\n",
    "\n",
    "#Incrementing each month for joining with transactions. Hence month in transaction will be joined by one previous month\n",
    "grouped_investment_aum_df['Mapping_Month']= grouped_investment_aum_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "grouped_advisor_aum_df['Mapping_Month']= grouped_advisor_aum_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "investment_exp_df['Mapping_Month']= investment_exp_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "aum_df['Mapping_Month']= aum_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "activity_df['Mapping_Month']= activity_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "transaction_df['Month'] =  transaction_df['Month'].astype(int).apply(lambda x : x+0)\n",
    "\n",
    "#merging data files\n",
    "final_transaction = pd.merge(transaction_df, grouped_investment_aum_df, left_on=[\"Month\",\"Unique_Investment_Id\"],right_on=[\"Mapping_Month\",\"Unique_Investment_Id\"], how=\"left\") \n",
    "final_transaction = final_transaction.rename(columns={'Month_x': 'Month', 'AUM': 'AUM_investor','Year_x' : 'Year', 'Counts' : 'Counts_investor', 'Shares': 'Shares_investor','Month_y':'Month_actual'})\n",
    "final_transaction = final_transaction.drop('Year_y', 1)\n",
    "\n",
    "test_transaction = pd.merge(test_df, grouped_investment_aum_df[grouped_investment_aum_df[\"Mapping_Month\"]== 13], on=\"Unique_Investment_Id\", how=\"left\")\n",
    "test_transaction = test_transaction.rename(columns={ 'AUM': 'AUM_investor','Counts' : 'Counts_investor', 'Shares': 'Shares_investor'})\n",
    "\n",
    "final_transaction = pd.merge(final_transaction, grouped_advisor_aum_df, left_on=[\"Month\",\"Unique_Advisor_Id\"],right_on=[\"Mapping_Month\",\"Unique_Advisor_Id\"], how=\"left\") \n",
    "final_transaction = final_transaction.rename(columns={'Month_x': 'Month', 'AUM': 'AUM_advisor','Year_x' : 'Year', 'Counts' : 'Counts_advisor', 'Shares': 'Shares_advisor','Mapping_Month_x': 'Mapping_Month'})\n",
    "final_transaction = final_transaction.drop(['Year_y','Mapping_Month_y','Month_y'], 1)\n",
    "\n",
    "test_transaction = pd.merge(test_transaction, grouped_advisor_aum_df[grouped_advisor_aum_df[\"Mapping_Month\"]== 13], on=\"Unique_Advisor_Id\", how=\"left\")\n",
    "test_transaction = test_transaction.rename(columns={ 'AUM': 'AUM_advisor','Counts' : 'Counts_advisor', 'Shares': 'Shares_advisor','Month_x':'Month','Mapping_Month_x':'Mapping_Month'})\n",
    "test_transaction = test_transaction.drop(['Year_x','Year_y','Mapping_Month_y','Month_y','Mapping_Month'], 1)\n",
    "\n",
    "investment_exp_df['investment_vehicle_segment']= investment_segment_df['investment_vehicle_segment']\n",
    "investment_exp_df= investment_exp_df[investment_exp_df['Year']=='2016']\n",
    "\n",
    "final_transaction_with_exp = pd.merge(final_transaction, investment_exp_df, left_on=[\"Month\",\"Unique_Investment_Id\"],right_on=[\"Mapping_Month\",\"Unique_Investment_Id\"], how=\"left\") \n",
    "final_transaction_with_exp = final_transaction_with_exp.rename(columns={'Month_x': 'Month', 'AUM': 'AUM_advisor','Year_x' : 'Year', 'Mapping_Month_x':'Mapping_Month'})\n",
    "final_transaction_with_exp = final_transaction_with_exp.drop(['Year_y','Mapping_Month_y','Month_y'], 1)\n",
    "\n",
    "test_transaction_with_exp = pd.merge(test_transaction, investment_exp_df[investment_exp_df[\"Mapping_Month\"]== 13 ], on=\"Unique_Investment_Id\", how=\"left\")\n",
    "\n",
    "#taking logs for scaling features\n",
    "final_transaction_with_exp['AUM_investor_log'] = (np.log(final_transaction_with_exp['AUM_investor'])).where(final_transaction_with_exp['AUM_investor']!=0)\n",
    "final_transaction_with_exp['Shares_investor_log'] = (np.log(final_transaction_with_exp['Shares_investor'])).where(final_transaction_with_exp['Shares_investor']!=0)\n",
    "\n",
    "final_transaction_with_exp['AUM_advisor_log'] = (np.log(final_transaction_with_exp['AUM_advisor'])).where(final_transaction_with_exp['AUM_advisor']!=0)\n",
    "final_transaction_with_exp['Shares_advisor_log'] = (np.log(final_transaction_with_exp['Shares_advisor'])).where(final_transaction_with_exp['Shares_advisor']!=0)\n",
    "\n",
    "\n",
    "test_transaction_with_exp['AUM_investor_log'] = (np.log(test_transaction_with_exp['AUM_investor'])).where(test_transaction_with_exp['AUM_investor']!=0)\n",
    "test_transaction_with_exp['Shares_investor_log'] = (np.log(test_transaction_with_exp['Shares_investor'])).where(test_transaction_with_exp['Shares_investor']!=0)\n",
    "\n",
    "test_transaction_with_exp['AUM_advisor_log'] = (np.log(test_transaction_with_exp['AUM_advisor'])).where(test_transaction_with_exp['AUM_advisor']!=0)\n",
    "test_transaction_with_exp['Shares_advisor_log'] = (np.log(test_transaction_with_exp['Shares_advisor'])).where(test_transaction_with_exp['Shares_advisor']!=0)\n",
    "\n",
    "\n",
    "\n",
    "#filtering columns from selected columns\n",
    "required_train_df = final_transaction_with_exp.filter(column_list)\n",
    "\n",
    "#filling NAs with mean\n",
    "required_train_df= required_train_df.fillna(required_train_df.median())\n",
    "\n",
    "#chaning transaction type to digits 0 and 1\n",
    "required_train_df['Transaction_Type']= required_train_df.apply(lambda x: 0 if x['Transaction_Type']== 'P' else 1, axis=1)\n",
    "\n",
    "#filtering columns from selected columns\n",
    "required_test_df = test_transaction_with_exp.filter(column_list)\n",
    "\n",
    "#filling NAs with median\n",
    "required_test_df=required_test_df.fillna(required_test_df.median())\n",
    "\n",
    "# Split the data into features and target label\n",
    "transaction_type = required_train_df['Transaction_Type']\n",
    "features_raw = required_train_df.drop('Transaction_Type', axis = 1)\n",
    "test_raw = required_test_df\n",
    "\n",
    "# label encoding for categorical features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "var_mod = column_list\n",
    "#var_mod.remove('Transaction_Type')\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    if i == 'Transaction_Type' \\\n",
    "    or i == 'Net Flows' \\\n",
    "    or i == 'AUM_investor_log' \\\n",
    "    or i == 'Shares_investor_log' \\\n",
    "    or i == 'AUM_advisor_log' \\\n",
    "    or i == 'Shares_advisor_log':\n",
    "        continue\n",
    "    features_raw[i] = le.fit_transform(features_raw[i])\n",
    "    test_raw[i] = le.fit_transform(test_raw[i])\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "# Split the 'features' and 'transaction_type' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_raw, transaction_type, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Show the results of the split\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "clf_A = LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "clf_B = LinearSVC(random_state=101)\n",
    "clf_C = GaussianNB()\n",
    "clf_Ada = AdaBoostClassifier()\n",
    "clf_Grad = GradientBoostingClassifier()\n",
    "clf_KNN = KNeighborsClassifier()\n",
    "clf_Dec = DecisionTreeClassifier()\n",
    "clf_SGD = SGDClassifier()\n",
    "\n",
    "\n",
    "# TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "n_train = len(y_train)\n",
    "samples_1 = int(n_train * 0.01)\n",
    "samples_10 = int(n_train * 0.1)\n",
    "samples_100 = n_train\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "#for clf in [clf_A, clf_B, clf_C, clf_Ada, clf_Grad,clf_KNN ,clf_Dec, clf_SGD]:\n",
    "for clf in [clf_A]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    for i, samples in enumerate([samples_100]):\n",
    "        results[clf_name][i] = \\\n",
    "        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "display(results)\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "best_clf= clf_A\n",
    "filename = 'logistic_regression_best_invesco.joblib.pkl'\n",
    "\n",
    "_ = joblib.dump(best_clf, filename, compress=9)\n",
    "\n",
    "clf_loaded = joblib.load(filename)\n",
    "\n",
    "\n",
    "pred = clf_loaded.predict(test_raw)\n",
    "pred_prob = clf_loaded.predict_proba(test_raw)\n",
    "\n",
    "pred_prob = pd.DataFrame(pred_prob[:,1],columns=[\"Propensity_Score\"])\n",
    "display(pred_prob[:5])\n",
    "\n",
    "pred_df= pd.DataFrame(pred,columns=[\"Redeem_Status\"])\n",
    "\n",
    "pred_df=pred_df.replace([0,1],['NO','YES'])\n",
    "pred_df.head()\n",
    "\n",
    "pred_df['Redeem_Status'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data frame for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_Advisor_Id</th>\n",
       "      <th>Unique_Investment_Id</th>\n",
       "      <th>Propensity_Score</th>\n",
       "      <th>Redeem_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000103</td>\n",
       "      <td>14147</td>\n",
       "      <td>0.979496</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000103</td>\n",
       "      <td>3534</td>\n",
       "      <td>0.645041</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000103</td>\n",
       "      <td>3651</td>\n",
       "      <td>0.233225</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000103</td>\n",
       "      <td>7668</td>\n",
       "      <td>0.800757</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000103</td>\n",
       "      <td>9339</td>\n",
       "      <td>0.392099</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_Advisor_Id  Unique_Investment_Id  Propensity_Score Redeem_Status\n",
       "0            1000103                 14147          0.979496           YES\n",
       "1            1000103                  3534          0.645041           YES\n",
       "2            1000103                  3651          0.233225            NO\n",
       "3            1000103                  7668          0.800757           YES\n",
       "4            1000103                  9339          0.392099            NO"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([test_df, pred_prob, pred_df], axis=1)\n",
    "\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.to_csv('test_data_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
