{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import numpy as np\n",
    "# Import supplementary visualization code visuals.py\n",
    "#import visuals as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting files and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data= \"/home/ubuntu/udacity/CodeGladiator/invesco/data\"\n",
    "\n",
    "transaction_file = \"Code-Gladiators-Transaction.csv\"\n",
    "#investment_exp_file = \"Code-Gladiators-InvestmentExperience.csv\"\n",
    "investment_exp_file = \"imputed_investment_exp.csv\"\n",
    "investment_segment = \"investment_vehicle_segment.csv\"\n",
    "aum_file = \"Code-Gladiators-AUM.csv\"\n",
    "activity_file = \"Code-Gladiators-Activity.csv\"\n",
    "\n",
    "test_file = \"test_data.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading csv files into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transaction_df = pd.read_csv(os.path.join(path_to_data, transaction_file))\n",
    "investment_exp_df = pd.read_csv(os.path.join(path_to_data, investment_exp_file))\n",
    "investment_segment_df = pd.read_csv(os.path.join(path_to_data, investment_segment))\n",
    "aum_df = pd.read_csv(os.path.join(path_to_data, aum_file))\n",
    "activity_df = pd.read_csv(os.path.join(path_to_data, activity_file))\n",
    "test_df = pd.read_csv(os.path.join(path_to_data,test_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Import two metrics from sklearn - fbeta_score and accuracy_score\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "# time.time() return time in seconds since the Epoch\n",
    "from time import time\n",
    "\n",
    "beta = 0.5\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size'\n",
    "    X_train = X_train[:sample_size]\n",
    "    y_train = y_train[:sample_size]\n",
    "    \n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train, y_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train, predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples\n",
    "    results['f_train'] = fbeta_score(y_train, predictions_train, beta=beta)\n",
    "        \n",
    "   # TODO: Compute F-score on the test set\n",
    "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=beta)\n",
    "       \n",
    "    # Success\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#column_list= ['AUM_investor_log','Counts_investor','Shares_investor_log','AUM_advisor_log','Shares_advisor_log','Rating','1 Yr % Rank','3 Yr % Rank','1 Yr Return','3 Yr Return','Net Flows','Morningstar_Category_Rating','Transaction_Type']\n",
    "\n",
    "\n",
    "column_list= [\n",
    "    'Transaction_Type',\n",
    "#    'Shares_investor',\n",
    "#    'AUM_investor',\n",
    "     'Counts_investor',\n",
    "#     'Counts_advisor',\n",
    "#\t'Shares_advisor',\n",
    "#\t'AUM_advisor',\n",
    "#\t'Morningstar Category',\n",
    "#\t'Investment',\n",
    "\t'Rating',\n",
    "\t'1 Yr % Rank',\n",
    "\t'3 Yr % Rank',\n",
    "#\t'5 Yr % Rank',\n",
    "#\t'10 Yr % Rank',\n",
    "\t'1 Yr Return',\n",
    "\t'3 Yr Return',\n",
    "#\t'5 Yr Return',\n",
    "#\t'10 Yr Return',\n",
    "#\t'1 Yr Excess Return vs Primary Ix',\n",
    "#\t'3 Yr Excess Return vs Primary Ix',\n",
    "#\t'5 Yr Excess Return vs Primary Ix',\n",
    "#\t'10 Yr Excess Return vs Primary Ix',\n",
    "#\t'1 Yr Excess Return vs Category Ix',\n",
    "#\t'3 Yr Excess Return vs Category Ix',\n",
    "#\t'5 Yr Excess Return vs Category Ix',\n",
    "#\t'10 Yr Excess Return vs Category Ix',\n",
    "\t'Net Flows',\n",
    "\t'Morningstar_Category_Rating',\n",
    "#\t'investment_vehicle_segment',\n",
    "\t'AUM_investor_log',\n",
    "\t'Shares_investor_log',\n",
    "\t'AUM_advisor_log',\n",
    "\t'Shares_advisor_log',\n",
    "\t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping data by unique advisor id and month. Final output will store sums of the assets under managements and shares for each advisor in particular month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:69: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:71: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:72: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:76: RuntimeWarning: divide by zero encountered in log\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel/__main__.py:79: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 100161 samples.\n",
      "Testing set has 25041 samples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#############################################################################\\n\\n\\n# TODO: Import the three supervised learning models from sklearn\\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\\nfrom sklearn.svm import LinearSVC, SVC\\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# TODO: Initialize the three models\\nclf_A = LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\\n          intercept_scaling=1, max_iter=100, multi_class=\\'ovr\\', n_jobs=1,\\n          penalty=\\'l2\\', random_state=None, solver=\\'newton-cg\\', tol=0.0001,\\n          verbose=0, warm_start=False)\\nclf_B = LinearSVC(random_state=101)\\nclf_C = GaussianNB()\\nclf_Ada = AdaBoostClassifier()\\nclf_Grad = GradientBoostingClassifier()\\nclf_KNN = KNeighborsClassifier()\\nclf_Dec = DecisionTreeClassifier()\\nclf_SGD = SGDClassifier()\\n\\n\\n# TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data\\nn_train = len(y_train)\\nsamples_1 = int(n_train * 0.01)\\nsamples_10 = int(n_train * 0.1)\\nsamples_100 = n_train\\n\\n# Collect results on the learners\\nresults = {}\\n#for clf in [clf_A, clf_B, clf_C, clf_Ada, clf_Grad,clf_KNN ,clf_Dec, clf_SGD]:\\nfor clf in [clf_A]:\\n    clf_name = clf.__class__.__name__\\n    results[clf_name] = {}\\n    for i, samples in enumerate([samples_100]):\\n        results[clf_name][i] =         train_predict(clf, samples, X_train, y_train, X_test, y_test)\\n        \\ndisplay(results)\\n\\n\\nfrom sklearn.externals import joblib\\n\\nbest_clf= clf_A\\nfilename = \\'logistic_regression_best_invesco.joblib.pkl\\'\\n\\n_ = joblib.dump(best_clf, filename, compress=9)\\n\\nclf_loaded = joblib.load(filename)\\n\\n\\npred = clf_loaded.predict(test_raw)\\npred_prob = clf_loaded.predict_proba(test_raw)\\n\\npred_prob = pd.DataFrame(pred_prob[:,1],columns=[\"Propensity_Score\"])\\ndisplay(pred_prob[:5])\\n\\npred_df= pd.DataFrame(pred,columns=[\"Redeem_Status\"])\\n\\npred_df=pred_df.replace([0,1],[\\'NO\\',\\'YES\\'])\\npred_df.head()\\n\\npred_df[\\'Redeem_Status\\'].value_counts()\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_df = pd.read_csv(os.path.join(path_to_data, transaction_file))\n",
    "investment_exp_df = pd.read_csv(os.path.join(path_to_data, investment_exp_file))\n",
    "investment_segment_df = pd.read_csv(os.path.join(path_to_data, investment_segment))\n",
    "aum_df = pd.read_csv(os.path.join(path_to_data, aum_file))\n",
    "activity_df = pd.read_csv(os.path.join(path_to_data, activity_file))\n",
    "test_df = pd.read_csv(os.path.join(path_to_data,test_file))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grouped_advisor_aum_df = aum_df.groupby(['Unique_Advisor_Id','Month']).agg({'AUM': 'sum','Shares':'sum','Unique_Investment_Id':'count'}).reset_index().rename(columns={'Unique_Investment_Id':'Counts'})\n",
    "grouped_investment_aum_df = aum_df.groupby(['Unique_Investment_Id','Month']).agg({'AUM': 'sum','Shares':'sum','Unique_Advisor_Id':'count'}).reset_index().rename(columns={'Unique_Advisor_Id':'Counts'})\n",
    "\n",
    "grouped_investment_aum_df['Year'],grouped_investment_aum_df['Month']=grouped_investment_aum_df['Month'].str.split(' /', 1).str\n",
    "grouped_advisor_aum_df['Year'],grouped_advisor_aum_df['Month']=grouped_advisor_aum_df['Month'].str.split(' /', 1).str\n",
    "\n",
    "transaction_df['Year'],transaction_df['Month']=transaction_df['Month'].str.split(' /', 1).str\n",
    "investment_exp_df['Year'],investment_exp_df['Month']=investment_exp_df['Month'].str.split(' /', 1).str\n",
    "aum_df['Year'],aum_df['Month']=aum_df['Month'].str.split(' /', 1).str\n",
    "activity_df['Year'],activity_df['Month']=activity_df['Month'].str.split(' /', 1).str\n",
    "\n",
    "grouped_investment_aum_df['Mapping_Month']= grouped_investment_aum_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "grouped_advisor_aum_df['Mapping_Month']= grouped_advisor_aum_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "investment_exp_df['Mapping_Month']= investment_exp_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "aum_df['Mapping_Month']= aum_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "activity_df['Mapping_Month']= activity_df['Month'].astype(int).apply(lambda x : x+1)\n",
    "\n",
    "transaction_df['Month'] =  transaction_df['Month'].astype(int).apply(lambda x : x+0)\n",
    "\n",
    "final_transaction = pd.merge(transaction_df, grouped_investment_aum_df, left_on=[\"Month\",\"Unique_Investment_Id\"],right_on=[\"Mapping_Month\",\"Unique_Investment_Id\"], how=\"left\") \n",
    "final_transaction = final_transaction.rename(columns={'Month_x': 'Month', 'AUM': 'AUM_investor','Year_x' : 'Year', 'Counts' : 'Counts_investor', 'Shares': 'Shares_investor','Month_y':'Month_actual'})\n",
    "final_transaction = final_transaction.drop('Year_y', 1)\n",
    "\n",
    "test_transaction = pd.merge(test_df, grouped_investment_aum_df[grouped_investment_aum_df[\"Mapping_Month\"]== 13], on=\"Unique_Investment_Id\", how=\"left\")\n",
    "test_transaction = test_transaction.rename(columns={ 'AUM': 'AUM_investor','Counts' : 'Counts_investor', 'Shares': 'Shares_investor'})\n",
    "\n",
    "final_transaction = pd.merge(final_transaction, grouped_advisor_aum_df, left_on=[\"Month\",\"Unique_Advisor_Id\"],right_on=[\"Mapping_Month\",\"Unique_Advisor_Id\"], how=\"left\") \n",
    "final_transaction = final_transaction.rename(columns={'Month_x': 'Month', 'AUM': 'AUM_advisor','Year_x' : 'Year', 'Counts' : 'Counts_advisor', 'Shares': 'Shares_advisor','Mapping_Month_x': 'Mapping_Month'})\n",
    "final_transaction = final_transaction.drop(['Year_y','Mapping_Month_y','Month_y'], 1)\n",
    "\n",
    "test_transaction = pd.merge(test_transaction, grouped_advisor_aum_df[grouped_advisor_aum_df[\"Mapping_Month\"]== 13], on=\"Unique_Advisor_Id\", how=\"left\")\n",
    "test_transaction = test_transaction.rename(columns={ 'AUM': 'AUM_advisor','Counts' : 'Counts_advisor', 'Shares': 'Shares_advisor','Month_x':'Month','Mapping_Month_x':'Mapping_Month'})\n",
    "test_transaction = test_transaction.drop(['Year_x','Year_y','Mapping_Month_y','Month_y','Mapping_Month'], 1)\n",
    "\n",
    "investment_exp_df['investment_vehicle_segment']= investment_segment_df['investment_vehicle_segment']\n",
    "investment_exp_df= investment_exp_df[investment_exp_df['Year']=='2016']\n",
    "\n",
    "final_transaction_with_exp = pd.merge(final_transaction, investment_exp_df, left_on=[\"Month\",\"Unique_Investment_Id\"],right_on=[\"Mapping_Month\",\"Unique_Investment_Id\"], how=\"left\") \n",
    "final_transaction_with_exp = final_transaction_with_exp.rename(columns={'Month_x': 'Month', 'AUM': 'AUM_advisor','Year_x' : 'Year', 'Mapping_Month_x':'Mapping_Month'})\n",
    "final_transaction_with_exp = final_transaction_with_exp.drop(['Year_y','Mapping_Month_y','Month_y'], 1)\n",
    "\n",
    "test_transaction_with_exp = pd.merge(test_transaction, investment_exp_df[investment_exp_df[\"Mapping_Month\"]== 13 ], on=\"Unique_Investment_Id\", how=\"left\")\n",
    "\n",
    "'''\n",
    "final_transaction_with_exp['AUM_investor_log'] = np.log(final_transaction_with_exp['AUM_investor'])\n",
    "final_transaction_with_exp['Shares_investor_log'] = np.log(final_transaction_with_exp['Shares_investor'])\n",
    "\n",
    "final_transaction_with_exp['AUM_advisor_log'] = np.log(final_transaction_with_exp['AUM_advisor'])\n",
    "final_transaction_with_exp['Shares_advisor_log'] = np.log(final_transaction_with_exp['Shares_advisor'])\n",
    "\n",
    "\n",
    "test_transaction_with_exp['AUM_investor_log'] = np.log(test_transaction_with_exp['AUM_investor'])\n",
    "test_transaction_with_exp['Shares_investor_log'] = np.log(test_transaction_with_exp['Shares_investor'])\n",
    "\n",
    "test_transaction_with_exp['AUM_advisor_log'] = np.log(test_transaction_with_exp['AUM_advisor'])\n",
    "test_transaction_with_exp['Shares_advisor_log'] = np.log(test_transaction_with_exp['Shares_advisor'])\n",
    "'''\n",
    "final_transaction_with_exp['AUM_investor_log'] = np.log(final_transaction_with_exp['AUM_investor']/final_transaction_with_exp['Counts_investor'])\n",
    "final_transaction_with_exp['Shares_investor_log'] = np.log(final_transaction_with_exp['Shares_investor']/final_transaction_with_exp['Counts_investor'])\n",
    "\n",
    "final_transaction_with_exp['AUM_advisor_log'] = np.log(final_transaction_with_exp['AUM_advisor']/final_transaction_with_exp['Counts_advisor'])\n",
    "final_transaction_with_exp['Shares_advisor_log'] = np.log(final_transaction_with_exp['Shares_advisor']/final_transaction_with_exp['Counts_advisor'])\n",
    "\n",
    "\n",
    "test_transaction_with_exp['AUM_investor_log'] = np.log(test_transaction_with_exp['AUM_investor']/test_transaction_with_exp['Counts_investor'])\n",
    "test_transaction_with_exp['Shares_investor_log'] = np.log(test_transaction_with_exp['Shares_investor']/test_transaction_with_exp['Counts_investor'])\n",
    "\n",
    "test_transaction_with_exp['AUM_advisor_log'] = np.log(test_transaction_with_exp['AUM_advisor']/test_transaction_with_exp['Counts_advisor'])\n",
    "test_transaction_with_exp['Shares_advisor_log'] = np.log(test_transaction_with_exp['Shares_advisor']/test_transaction_with_exp['Counts_advisor'])\n",
    "final_transaction_with_exp= final_transaction_with_exp.dropna()\n",
    "\n",
    "\n",
    "required_train_df = final_transaction_with_exp.filter(column_list)\n",
    "\n",
    "required_train_df['Transaction_Type']= required_train_df.apply(lambda x: 0 if x['Transaction_Type']== 'P' else 1, axis=1)\n",
    "\n",
    "required_test_df = test_transaction_with_exp.filter(column_list)\n",
    "\n",
    "\n",
    "#required_test_df['Rating'] = required_test_df['Rating'].astype(float)\n",
    "\n",
    "required_test_df=required_test_df.fillna(required_test_df.median())\n",
    "\n",
    "# Split the data into features and target label\n",
    "transaction_type = required_train_df['Transaction_Type']\n",
    "features_raw = required_train_df.drop('Transaction_Type', axis = 1)\n",
    "test_raw = required_test_df\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "var_mod = column_list\n",
    "#var_mod.remove('Transaction_Type')\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    if i == 'Transaction_Type':\n",
    "        continue\n",
    "    features_raw[i] = le.fit_transform(features_raw[i])\n",
    "    test_raw[i] = le.fit_transform(test_raw[i])\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split the 'features' and 'transaction_type' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_raw, transaction_type, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Show the results of the split\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "clf_A = LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "clf_B = LinearSVC(random_state=101)\n",
    "clf_C = GaussianNB()\n",
    "clf_Ada = AdaBoostClassifier()\n",
    "clf_Grad = GradientBoostingClassifier()\n",
    "clf_KNN = KNeighborsClassifier()\n",
    "clf_Dec = DecisionTreeClassifier()\n",
    "clf_SGD = SGDClassifier()\n",
    "\n",
    "\n",
    "# TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "n_train = len(y_train)\n",
    "samples_1 = int(n_train * 0.01)\n",
    "samples_10 = int(n_train * 0.1)\n",
    "samples_100 = n_train\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "#for clf in [clf_A, clf_B, clf_C, clf_Ada, clf_Grad,clf_KNN ,clf_Dec, clf_SGD]:\n",
    "for clf in [clf_Grad]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    for i, samples in enumerate([samples_100]):\n",
    "        results[clf_name][i] = \\\n",
    "        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "display(results)\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "best_clf= clf_A\n",
    "filename = 'logistic_regression_best_invesco.joblib.pkl'\n",
    "\n",
    "_ = joblib.dump(best_clf, filename, compress=9)\n",
    "\n",
    "clf_loaded = joblib.load(filename)\n",
    "\n",
    "\n",
    "pred = clf_loaded.predict(test_raw)\n",
    "pred_prob = clf_loaded.predict_proba(test_raw)\n",
    "\n",
    "pred_prob = pd.DataFrame(pred_prob[:,1],columns=[\"Propensity_Score\"])\n",
    "display(pred_prob[:5])\n",
    "\n",
    "pred_df= pd.DataFrame(pred,columns=[\"Redeem_Status\"])\n",
    "\n",
    "pred_df=pred_df.replace([0,1],['NO','YES'])\n",
    "pred_df.head()\n",
    "\n",
    "pred_df['Redeem_Status'].value_counts()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for blending.\n",
      "0 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "1 LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "2 DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "3 KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "4 GaussianNB(priors=None)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "5 GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=6,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=50, presort='auto', random_state=None,\n",
      "              subsample=0.5, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Blending.\n",
      "Accuracy score on testing data: 0.6947\n",
      "F-score on testing data: 0.7786\n"
     ]
    }
   ],
   "source": [
    "X = X_train.as_matrix()\n",
    "y= y_train.as_matrix()\n",
    "X_submission = X_test.as_matrix()\n",
    "y_check_test = y_test.as_matrix()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "np.random.seed(0)  # seed to shuffle the train set\n",
    "\n",
    "n_folds = 10\n",
    "verbose = True\n",
    "shuffle = False\n",
    "\n",
    "\n",
    "if shuffle:\n",
    "    idx = np.random.permutation(y.size)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "skf = list(StratifiedKFold(y, n_folds))\n",
    "\n",
    "clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        LogisticRegression(),\n",
    "        DecisionTreeClassifier(),\n",
    "#        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        KNeighborsClassifier(),\n",
    "        GaussianNB(),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "print (\"Creating train and test sets for blending.\")\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    print(j, clf)\n",
    "    dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print(\"Fold\", i)\n",
    "        \n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        X_test = X[test]\n",
    "        y_test = y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:, 1]\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "\n",
    "\n",
    "print (\"Blending.\")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(dataset_blend_train, y)\n",
    "#y_submission = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    "y_submission = clf.predict(dataset_blend_test)\n",
    "\n",
    "\n",
    "print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_check_test, y_submission)))\n",
    "print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_check_test, y_submission, beta = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for blending.\n",
      "0 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "1 LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "2 DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "3 KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "4 GaussianNB(priors=None)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "5 GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=6,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=50, presort='auto', random_state=None,\n",
      "              subsample=0.5, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Blending.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "X = features_raw.as_matrix()\n",
    "y= transaction_type.as_matrix()\n",
    "X_submission = test_raw.as_matrix()\n",
    "#y_check_test = y_test.as_matrix()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "np.random.seed(0)  # seed to shuffle the train set\n",
    "\n",
    "n_folds = 10\n",
    "verbose = True\n",
    "shuffle = False\n",
    "\n",
    "\n",
    "if shuffle:\n",
    "    idx = np.random.permutation(y.size)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "skf = list(StratifiedKFold(y, n_folds))\n",
    "\n",
    "clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        LogisticRegression(),\n",
    "        DecisionTreeClassifier(),\n",
    "#        ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        KNeighborsClassifier(),\n",
    "        GaussianNB(),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "print (\"Creating train and test sets for blending.\")\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    print(j, clf)\n",
    "    dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print(\"Fold\", i)\n",
    "        \n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        X_test = X[test]\n",
    "        y_test = y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:, 1]\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "\n",
    "\n",
    "print (\"Blending.\")\n",
    "clf = LogisticRegression()\n",
    "clf.fit(dataset_blend_train, y)\n",
    "y_submission_prob = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    "y_submission = clf.predict(dataset_blend_test)\n",
    "\n",
    "print('done')\n",
    "\n",
    "#print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_check_test, y_submission)))\n",
    "#print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_check_test, y_submission, beta = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Propensity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.449298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.580018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.423865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.569235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.607528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Propensity_Score\n",
       "0          0.449298\n",
       "1          0.580018\n",
       "2          0.423865\n",
       "3          0.569235\n",
       "4          0.607528"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NO     6034\n",
       "YES    2680\n",
       "Name: Redeem_Status, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_submission_prob = clf.predict_proba(dataset_blend_test)\n",
    "y_submission = clf.predict(dataset_blend_test)\n",
    "\n",
    "pred_prob= y_submission_prob\n",
    "pred = y_submission\n",
    "#pred = clf_loaded.predict(test_raw)\n",
    "#pred_prob = clf_loaded.predict_proba(test_raw)\n",
    "\n",
    "pred_prob = pd.DataFrame(pred_prob[:,1],columns=[\"Propensity_Score\"])\n",
    "display(pred_prob[:5])\n",
    "\n",
    "pred_df= pd.DataFrame(pred,columns=[\"Redeem_Status\"])\n",
    "\n",
    "pred_df=pred_df.replace([0,1],['NO','YES'])\n",
    "pred_df.head()\n",
    "\n",
    "pred_df['Redeem_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_Advisor_Id</th>\n",
       "      <th>Unique_Investment_Id</th>\n",
       "      <th>Propensity_Score</th>\n",
       "      <th>Redeem_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000103</td>\n",
       "      <td>14147</td>\n",
       "      <td>0.786417</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000103</td>\n",
       "      <td>3534</td>\n",
       "      <td>0.415081</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000103</td>\n",
       "      <td>3651</td>\n",
       "      <td>0.169032</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000103</td>\n",
       "      <td>7668</td>\n",
       "      <td>0.387780</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000103</td>\n",
       "      <td>9339</td>\n",
       "      <td>0.508985</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_Advisor_Id  Unique_Investment_Id  Propensity_Score Redeem_Status\n",
       "0            1000103                 14147          0.786417           YES\n",
       "1            1000103                  3534          0.415081            NO\n",
       "2            1000103                  3651          0.169032            NO\n",
       "3            1000103                  7668          0.387780            NO\n",
       "4            1000103                  9339          0.508985           YES"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([test_df, pred_prob, pred_df], axis=1)\n",
    "\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv('test_data_v3.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
